{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Captioning Training Notebook\n",
        "## CNN (ResNet152) + LSTM + Attention\n",
        "### Dataset: Flickr30k via Kaggle\n",
        "\n",
        "This notebook trains a custom image captioning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Download Flickr30k from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already exists at flickr30k_data\n",
            "Images: flickr30k_data/flickr30k_images/flickr30k_images\n",
            "Captions: flickr30k_data/flickr30k_images/results.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Download Flickr30k from Kaggle (adityajn105/flickr30k)\n",
        "DATASET_DIR = \"flickr30k_data\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    print(\"Downloading Flickr30k from Kaggle...\")\n",
        "    !kaggle datasets download -d adityajn105/flickr30k -p {DATASET_DIR}\n",
        "    \n",
        "    # Unzip\n",
        "    print(\"Extracting...\")\n",
        "    for f in os.listdir(DATASET_DIR):\n",
        "        if f.endswith('.zip'):\n",
        "            with zipfile.ZipFile(os.path.join(DATASET_DIR, f), 'r') as z:\n",
        "                z.extractall(DATASET_DIR)\n",
        "            os.remove(os.path.join(DATASET_DIR, f))\n",
        "    print(\"Done!\")\n",
        "else:\n",
        "    print(f\"Dataset already exists at {DATASET_DIR}\")\n",
        "\n",
        "# Find images and captions\n",
        "IMAGES_DIR = os.path.join(DATASET_DIR, \"flickr30k_images\", \"flickr30k_images\")\n",
        "CAPTIONS_FILE = os.path.join(DATASET_DIR, \"flickr30k_images\", \"results.csv\")\n",
        "\n",
        "print(f\"Images: {IMAGES_DIR}\")\n",
        "print(f\"Captions: {CAPTIONS_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = os.getcwd()\n",
        "FEATURES_DIR = os.path.join(BASE_DIR, \"features_flickr30k\")\n",
        "MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"backend\", \"custom_caption_model.pth\")\n",
        "VOCAB_SAVE_PATH = os.path.join(BASE_DIR, \"backend\", \"vocab.json\")\n",
        "\n",
        "# Hyperparameters\n",
        "EMBED_DIM = 256\n",
        "ATTENTION_DIM = 256\n",
        "DECODER_DIM = 512\n",
        "ENCODER_DIM = 2048\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 4e-4\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 20\n",
        "MAX_CAPTION_LEN = 50\n",
        "MIN_WORD_FREQ = 5\n",
        "PATIENCE = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading captions...\n",
            "Loaded captions for 31783 images\n"
          ]
        }
      ],
      "source": [
        "def clean_caption(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z ]+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "print(\"Loading captions...\")\n",
        "raw_captions = defaultdict(list)\n",
        "\n",
        "with open(CAPTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f, delimiter='|')\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        if len(row) >= 3:\n",
        "            img_id = row[0].strip()\n",
        "            caption = row[2].strip() if len(row) > 2 else row[1].strip()\n",
        "            raw_captions[img_id].append(caption)\n",
        "\n",
        "print(f\"Loaded captions for {len(raw_captions)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocabulary...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "429af6b3964b485f929e15beb1c75b98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/31783 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid images: 31783\n",
            "Unique words: 18081\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary\n",
        "print(\"Building vocabulary...\")\n",
        "counter = Counter()\n",
        "all_captions = {}\n",
        "\n",
        "for img_id, caps in tqdm(raw_captions.items(), desc=\"Processing\"):\n",
        "    img_path = os.path.join(IMAGES_DIR, img_id)\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "    \n",
        "    processed = []\n",
        "    for cap in caps:\n",
        "        cleaned = clean_caption(cap)\n",
        "        if cleaned:\n",
        "            final = f\"<start> {cleaned} <end>\"\n",
        "            processed.append(final)\n",
        "            counter.update(final.split())\n",
        "    \n",
        "    if processed:\n",
        "        all_captions[img_id] = processed\n",
        "\n",
        "print(f\"Valid images: {len(all_captions)}\")\n",
        "print(f\"Unique words: {len(counter)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 7611\n",
            "Saved to /home/sebastian/Desktop/UNI/Master Anul 1/Sem 1/IBD/ImageCaption/image_caption/backend/vocab.json\n"
          ]
        }
      ],
      "source": [
        "# Create vocabulary\n",
        "words = [w for w, cnt in counter.items() if cnt >= MIN_WORD_FREQ and w not in {\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"}]\n",
        "words = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"] + words\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "\n",
        "# Save\n",
        "os.makedirs(os.path.dirname(VOCAB_SAVE_PATH), exist_ok=True)\n",
        "with open(VOCAB_SAVE_PATH, 'w') as f:\n",
        "    json.dump({'word2idx': word2idx, 'idx2word': {str(k): v for k, v in idx2word.items()}}, f)\n",
        "print(f\"Saved to {VOCAB_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 27015, Val: 4768\n"
          ]
        }
      ],
      "source": [
        "all_ids = list(all_captions.keys())\n",
        "random.seed(42)\n",
        "random.shuffle(all_ids)\n",
        "\n",
        "train_ids = all_ids[:int(0.85 * len(all_ids))]\n",
        "val_ids = all_ids[int(0.85 * len(all_ids)):]\n",
        "\n",
        "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Extraction (ResNet152)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ResNet152...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /home/sebastian/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 230M/230M [00:23<00:00, 10.1MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading ResNet152...\")\n",
        "resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n",
        "modules = list(resnet.children())[:-2]\n",
        "encoder = nn.Sequential(*modules).to(DEVICE).eval()\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def extract_features(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img_transform(img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        return encoder(img).squeeze().cpu().numpy()\n",
        "\n",
        "print(\"Encoder ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features to /home/sebastian/Desktop/UNI/Master Anul 1/Sem 1/IBD/ImageCaption/image_caption/features_flickr30k...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f73a6b11aa8c49eeb71f22989f76e5c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Features:   0%|          | 0/31783 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "print(f\"Extracting features to {FEATURES_DIR}...\")\n",
        "\n",
        "for img_id in tqdm(all_captions.keys(), desc=\"Features\"):\n",
        "    save_path = os.path.join(FEATURES_DIR, img_id.replace(\".jpg\", \".npy\"))\n",
        "    if not os.path.exists(save_path):\n",
        "        try:\n",
        "            feat = extract_features(os.path.join(IMAGES_DIR, img_id))\n",
        "            np.save(save_path, feat)\n",
        "        except Exception as e:\n",
        "            print(f\"Error {img_id}: {e}\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dataset & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 4222, Val batches: 745\n"
          ]
        }
      ],
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, ids, captions, features_dir, word2idx, max_len=MAX_CAPTION_LEN):\n",
        "        self.data = []\n",
        "        for img_id in ids:\n",
        "            fp = os.path.join(features_dir, img_id.replace(\".jpg\", \".npy\"))\n",
        "            if os.path.exists(fp):\n",
        "                for cap in captions[img_id]:\n",
        "                    self.data.append((fp, cap))\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self): return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        fp, cap = self.data[idx]\n",
        "        feat = torch.tensor(np.load(fp), dtype=torch.float32)\n",
        "        \n",
        "        indices = [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in cap.split()]\n",
        "        indices = indices[:self.max_len] + [self.word2idx[\"<pad>\"]] * max(0, self.max_len - len(indices))\n",
        "        \n",
        "        return feat, torch.tensor(indices[:self.max_len], dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(CaptionDataset(train_ids, all_captions, FEATURES_DIR, word2idx), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(CaptionDataset(val_ids, all_captions, FEATURES_DIR, word2idx), batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, att_dim):\n",
        "        super().__init__()\n",
        "        self.enc_att = nn.Linear(enc_dim, att_dim)\n",
        "        self.dec_att = nn.Linear(dec_dim, att_dim)\n",
        "        self.full_att = nn.Linear(att_dim, 1)\n",
        "    \n",
        "    def forward(self, enc, dec):\n",
        "        att = self.full_att(torch.relu(self.enc_att(enc) + self.dec_att(dec).unsqueeze(1)))\n",
        "        alpha = torch.softmax(att, dim=1)\n",
        "        return (enc * alpha).sum(dim=1), alpha\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, att_dim, emb_dim, dec_dim, vocab_size, enc_dim=2048, drop=0.5):\n",
        "        super().__init__()\n",
        "        self.enc_dim = enc_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention = Attention(enc_dim, dec_dim, att_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.dropout = nn.Dropout(drop)\n",
        "        self.lstm = nn.LSTMCell(emb_dim + enc_dim, dec_dim)\n",
        "        self.init_h = nn.Linear(enc_dim, dec_dim)\n",
        "        self.init_c = nn.Linear(enc_dim, dec_dim)\n",
        "        self.fc = nn.Linear(dec_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, enc_out, caps):\n",
        "        B = enc_out.size(0)\n",
        "        enc_out = enc_out.permute(0,2,3,1).view(B, -1, self.enc_dim)\n",
        "        \n",
        "        emb = self.embedding(caps)\n",
        "        mean = enc_out.mean(1)\n",
        "        h, c = self.init_h(mean), self.init_c(mean)\n",
        "        \n",
        "        preds = torch.zeros(B, caps.size(1)-1, self.vocab_size).to(DEVICE)\n",
        "        alphas = torch.zeros(B, caps.size(1)-1, 49).to(DEVICE)\n",
        "        \n",
        "        for t in range(caps.size(1)-1):\n",
        "            ctx, a = self.attention(enc_out, h)\n",
        "            h, c = self.lstm(torch.cat([emb[:,t], ctx], 1), (h, c))\n",
        "            preds[:,t] = self.fc(self.dropout(h))\n",
        "            alphas[:,t] = a.squeeze(2)\n",
        "        \n",
        "        return preds, alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: 14,378,428\n"
          ]
        }
      ],
      "source": [
        "model = Decoder(ATTENTION_DIM, EMBED_DIM, DECODER_DIM, VOCAB_SIZE).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
        "\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3768f3afcbf47ce8aa78c06320a894a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 | Train: 3.8142 | Val: 3.2720\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8176ad30f7db46f4a264a63af74520f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 | Train: 3.1915 | Val: 3.0651\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71b865e07b345ff9bda21ade5bc02f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 | Train: 2.9765 | Val: 2.9862\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76f54e958a8341b9818bb04da0a31498",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 | Train: 2.8321 | Val: 2.9488\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55b91094baf646508b2fb194f93f2070",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 | Train: 2.7216 | Val: 2.9297\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57c8f029ef5b41e58cdd681050010240",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 | Train: 2.6289 | Val: 2.9256\n",
            "  Saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a84d6027329a42278329aec049fce3f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 | Train: 2.5498 | Val: 2.9299\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ad0ddeed6bd44b381a56efa6e4527ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 | Train: 2.4780 | Val: 2.9423\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1c41d75749742b0a69ffbb0664b3c83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 | Train: 2.4153 | Val: 2.9596\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f45900a31ad14c30ae5bae8fc631a1f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 | Train: 2.2800 | Val: 2.9698\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a978d8540ff4349857f50ff2f9729f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4222 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 | Train: 2.2206 | Val: 2.9903\n",
            "Early stopping!\n",
            "\n",
            "Done! Best loss: 2.9256\n",
            "Model: /home/sebastian/Desktop/UNI/Master Anul 1/Sem 1/IBD/ImageCaption/image_caption/backend/custom_caption_model.pth\n"
          ]
        }
      ],
      "source": [
        "best_loss = float('inf')\n",
        "patience_cnt = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for imgs, caps in tqdm(train_loader, leave=False):\n",
        "        imgs, caps = imgs.to(DEVICE), caps.to(DEVICE)\n",
        "        out, alphas = model(imgs, caps)\n",
        "        loss = criterion(out.view(-1, VOCAB_SIZE), caps[:,1:].reshape(-1))\n",
        "        loss += ((1 - alphas.sum(1))**2).mean()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # Val\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, caps in val_loader:\n",
        "            imgs, caps = imgs.to(DEVICE), caps.to(DEVICE)\n",
        "            out, _ = model(imgs, caps)\n",
        "            val_loss += criterion(out.view(-1, VOCAB_SIZE), caps[:,1:].reshape(-1)).item()\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "    \n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        patience_cnt = 0\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'vocab_size': VOCAB_SIZE,\n",
        "                    'embed_dim': EMBED_DIM, 'attention_dim': ATTENTION_DIM,\n",
        "                    'decoder_dim': DECODER_DIM, 'encoder_dim': ENCODER_DIM}, MODEL_SAVE_PATH)\n",
        "        print(\"  Saved!\")\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= PATIENCE:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nDone! Best loss: {best_loss:.4f}\")\n",
        "print(f\"Model: {MODEL_SAVE_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
